{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99cd0931",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!pip install -q unsloth accelerate datasets peft bitsandbytes trl\n",
    "from unsloth import FastLanguageModel\n",
    "from transformers import AutoTokenizer\n",
    "import torch\n",
    "\n",
    "\n",
    "model_name = \"ypavanr/llama3-taxbot-lora\"\n",
    "\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = model_name,\n",
    "    max_seq_length = 4096,\n",
    "    dtype = torch.float16,\n",
    "    load_in_4bit = True,    \n",
    ")\n",
    "\n",
    "\n",
    "FastLanguageModel.for_inference(model)  # Enable native 2x faster inference\n",
    "\n",
    "prompt = \"<|system|>\\nYou are a helpful tax advisor AI.\\n<|user|>\\nlist all the deductions\\n<|assistant|>\\n\"\n",
    "\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "\n",
    "outputs = model.generate(**inputs, max_new_tokens=512, temperature=0.7, top_p=0.9)\n",
    "\n",
    "\n",
    "response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "if \"<|assistant|>\" in response:\n",
    "    response = response.split(\"<|assistant|>\")[1].split(\"<|user|>\")[0].strip()\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "469ab440",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!pip install fastapi uvicorn pyngrok transformers accelerate bitsandbytes peft torch --quiet\n",
    "!pip install nest_asyncio --quiet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "755fe9f5",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from fastapi import FastAPI\n",
    "from pydantic import BaseModel\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "import uvicorn\n",
    "from pyngrok import ngrok\n",
    "import nest_asyncio\n",
    "from fastapi.middleware.cors import CORSMiddleware\n",
    "\n",
    "ngrok.set_auth_token(NGROK_AUTH_TOKEN)\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "app = FastAPI()\n",
    "app.add_middleware(\n",
    "    CORSMiddleware,\n",
    "    allow_origins=[\"*\"],  # or [\"*\"] to allow all\n",
    "    allow_methods=[\"*\"],\n",
    "    allow_headers=[\"*\"],\n",
    ")\n",
    "\n",
    "class PromptRequest(BaseModel):\n",
    "    prompt: str\n",
    "\n",
    "@app.post(\"/generate\")\n",
    "def generate(request: PromptRequest):\n",
    "    full_prompt = f\"<|system|>\\nYou are a helpful tax advisor AI.\\n<|user|>\\n{request.prompt}\\n<|assistant|>\\n\"\n",
    "    inputs = tokenizer(full_prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "    outputs = model.generate(**inputs, max_new_tokens=512, temperature=0.7, top_p=0.9)\n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "  \n",
    "    if \"<|assistant|>\" in response:\n",
    "        response = response.split(\"<|assistant|>\")[1].split(\"<|user|>\")[0].strip()\n",
    "    return {\"response\": response}\n",
    "\n",
    "\n",
    "public_url = ngrok.connect(8000)\n",
    "print(f\"ðŸ”— Public URL: {public_url}\")\n",
    "\n",
    "\n",
    "uvicorn.run(app, port=8000)                     "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
