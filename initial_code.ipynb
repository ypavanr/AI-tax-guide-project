{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f522390",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "login()  # paste your HF token here (https://huggingface.co/settings/tokens)\n",
    "# 1. Install Unsloth and dependencies\n",
    "!pip install -q unsloth accelerate datasets peft bitsandbytes trl\n",
    "\n",
    "# 2. Load Unsloth with Meta-Llama-3 8B Instruct\n",
    "from unsloth import FastLanguageModel\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"unsloth/llama-3-3b-Instruct-bnb-4bit\",  # Unsloth quantized\n",
    "    max_seq_length = 2048,\n",
    "    dtype = None,\n",
    "    load_in_4bit = True,\n",
    ")\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# 3. Upload your tax dataset\n",
    "from google.colab import files\n",
    "uploaded = files.upload()  # Upload itr1_full_finetune_dataset.jsonl\n",
    "\n",
    "# 4. Load and prepare dataset\n",
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"json\", data_files=\"itr1_full_finetune_dataset.jsonl\", split=\"train\")\n",
    "\n",
    "def format_prompt(example):\n",
    "    return {\n",
    "        \"prompt\": f\"<|system|>\\nYou are a helpful tax advisor AI.\\n<|user|>\\n{example['instruction']}\\n<|assistant|>\\n\",\n",
    "        \"output\": example[\"output\"]\n",
    "    }\n",
    "\n",
    "dataset = dataset.map(format_prompt)\n",
    "\n",
    "# 5. Prepare model for LoRA fine-tuning\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r=8,\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                    \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0,\n",
    "    use_gradient_checkpointing=True,\n",
    "    random_state=42,\n",
    "    use_rslora=False,\n",
    "    loftq_config=None,\n",
    ")\n",
    "\n",
    "# 6. Fine-tune with TRL (SFTTrainer)\n",
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = dataset,\n",
    "    dataset_text_field = \"prompt\",\n",
    "    max_seq_length = 2048,\n",
    "    dataset_num_proc = 2,\n",
    "    packing = False,\n",
    "    args = TrainingArguments(\n",
    "        per_device_train_batch_size = 2,\n",
    "        gradient_accumulation_steps = 4,\n",
    "        num_train_epochs = 3,\n",
    "        learning_rate = 2e-4,\n",
    "        fp16 = True,\n",
    "        logging_steps = 10,\n",
    "        output_dir = \"llama3-taxbot-lora\",\n",
    "        save_strategy = \"epoch\",\n",
    "        save_total_limit = 1,\n",
    "    )\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "# 7. Save fine-tuned model\n",
    "model.save_pretrained(\"llama3-3B-taxbot-lora\")\n",
    "tokenizer.save_pretrained(\"llama3-3B-taxbot-lora\")\n",
    "model.save_pretrained(\"llama3-taxbot-lora\", save_adapter=True)\n",
    "from huggingface_hub import HfApi\n",
    "from peft import PeftModel\n",
    "\n",
    "api = HfApi()\n",
    "api.upload_folder(\n",
    "    folder_path=\"/content/llama3-3B-taxbot-lora\",\n",
    "    repo_id=\"ypavanr/llama3-3B-taxbot-lora\",\n",
    "    repo_type=\"model\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c920de26",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "prompt = \"<|system|>\\nYou are a helpful tax advisor AI.\\n<|user|>\\nlist all the deductions\\n<|assistant|>\\n\"\n",
    "\n",
    "# 2. Tokenize the input\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "# 3. Generate output\n",
    "outputs = model.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens=300,\n",
    "    do_sample=True,\n",
    "    temperature=0.7\n",
    ")\n",
    "\n",
    "# 4. Decode the generated output\n",
    "response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "if \"<|assistant|>\" in response:\n",
    "    response = response.split(\"<|assistant|>\")[1].split(\"<|user|>\")[0].strip()\n",
    "# 5. Print the result\n",
    "print(response)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
